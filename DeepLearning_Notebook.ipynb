{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2efGTzDoaRE9",
    "ExecuteTime": {
     "end_time": "2024-02-12T16:39:02.520406Z",
     "start_time": "2024-02-12T16:38:57.026012800Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses, Input, backend\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plain Autoencoder"
   ],
   "metadata": {
    "id": "Qr3dXwNlaqtJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Autoencoder(Model):\n",
    "  def __init__(self, encoder_input_shape, n_layers_encoder,\n",
    "               n_layers_decoder, output_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.encoder_input_shape = (28, 28, 1)\n",
    "    self.output_dim = 2\n",
    "\n",
    "    ### The encoder\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Input(shape=self.encoder_input_shape, name='encoder_input'),\n",
    "      layers.Conv2D(filters = 32, kernel_size = (3, 3), strides = 1, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 1, padding = 'same', activation = 'LeakyReLU'),\n",
    "    ])\n",
    "\n",
    "    ### The decoder\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Conv2DTranspose(filters = 64, kernel_size = (3, 3), strides=1, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2DTranspose(filters = 64, kernel_size = (3, 3), strides=2, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2DTranspose(filters = 32, kernel_size = (3, 3), strides=2, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2DTranspose(filters = 1,  kernel_size = (3, 3), strides=1, padding = 'same', activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    shape_before_flattening = backend.int_shape(encoded)[1:]\n",
    "    encoded = layers.Flatten()(encoded)\n",
    "    encoder_output = layers.Dense(self.output_dim)(encoded)\n",
    "    self.mu = layers.Dense(self.output_dim, name = 'mu')(encoded)\n",
    "    self.log_var = layers.Dense(self.output_dim, name = 'log_var')(encoded)\n",
    "\n",
    "    decoder_input = layers.Input(shape=self.output_dim, name = 'decoder_input')\n",
    "    decoded = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "    decoded = layers.Reshape(shape_before_flattening)(encoder_output)\n",
    "    decoder_output = self.decoder(decoded)\n",
    "\n",
    "    return decoder_output\n",
    "\n",
    "\n",
    "input_shape = x_test.shape[1:]\n",
    "n_layers_encoder = 4\n",
    "n_layers_decoder = 4\n",
    "output_dim = 2\n",
    "\n",
    "# Joining the Encoder to the Decoder\n",
    "autoencoder = Autoencoder(input_shape, n_layers_encoder, n_layers_decoder,\n",
    "                          output_dim)\n"
   ],
   "metadata": {
    "id": "AViwykXTaxtK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def r_loss(y_true, y_pred):\n",
    "  return backend.mean(backend.square(y_true - y_pred), axis = [1, 2, 3])\n",
    "\n",
    "autoencoder.compile(optimizer = 'adam', loss = r_loss)"
   ],
   "metadata": {
    "id": "71sX7wUPw7TX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "28 x 28 image\n",
    "3 x 3 filter = 9 params per filter\n",
    "1 bias term per filter + 9 = 10 params per filter\n",
    "32 filters = 10 x 32 params = 320 params\n",
    "the result is a 28 x 28 x 32 (each filter picks out a particular feature)\n",
    "\n",
    "stack the filters\n",
    "28 x 28 x 32 input shape\n",
    "64 new filters x 9 params per filter = 576 params\n",
    "576 params per channel (filtered image)\n",
    "576 x 32 channels = 18432 params\n",
    "1 bias term per new filter = 1 x 64 = 64 params\n",
    "18496 params"
   ],
   "metadata": {
    "id": "otI9D0myFuEC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variational Autoencoder\n",
    "\n"
   ],
   "metadata": {
    "id": "EBhUWGULabd2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from numpy.core.fromnumeric import nonzero\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, encoder_input_shape, n_layers_encoder,\n",
    "               n_layers_decoder, output_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.encoder_input_shape = (28, 28, 1)\n",
    "    self.output_dim = 2\n",
    "    self.shape_before_flattening = (7, 7, 64)\n",
    "    self.dense1 = layers.Dense(self.output_dim, name = 'mu')\n",
    "    self.dense2 = layers.Dense(self.output_dim, name = 'log_var')\n",
    "    self.dense3 = layers.Dense(np.prod(self.shape_before_flattening))\n",
    "\n",
    "    ### The encoder\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Input(shape=self.encoder_input_shape, name='encoder_input'),\n",
    "      layers.Conv2D(filters = 32, kernel_size = (3, 3), strides = 1, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 1, padding = 'same', activation = 'LeakyReLU'),\n",
    "    ])\n",
    "\n",
    "    ### The decoder\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Conv2DTranspose(filters = 64, kernel_size = (3, 3), strides=1, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2DTranspose(filters = 64, kernel_size = (3, 3), strides=2, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2DTranspose(filters = 32, kernel_size = (3, 3), strides=2, padding = 'same', activation = 'LeakyReLU'),\n",
    "      layers.Conv2DTranspose(filters = 1,  kernel_size = (3, 3), strides=1, padding = 'same', activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)                                      # 32x28x28x1\n",
    "    self.shape_before_flattening = backend.int_shape(encoded)[1:]  # 32x7x7x64\n",
    "    print(self.shape_before_flattening)\n",
    "    encoded = layers.Flatten()(encoded)                            # 32x3136\n",
    "    self.mu = self.dense1(encoded)                                 # 32x2 ; each image gets a pseudo mean value (3136 values translated into a 2 coords (the mean))\n",
    "    self.log_var = self.dense2(encoded)                            # 32x2 ; and a pseudo log_var value (between -inf and inf)\n",
    "\n",
    "    def sampling(args):\n",
    "      # Samples a random point from the normal distribution (for a specific mu and log_var)\n",
    "      mu, log_var = args\n",
    "      epsilon = backend.random_normal(shape = backend.shape(mu), mean = 0, stddev = 1.) # random point in normal distribution\n",
    "      return mu + backend.exp(log_var / 2) * epsilon\n",
    "\n",
    "    encoder_output = layers.Lambda(sampling, name = 'encoder_output')([self.mu, self.log_var]) # 32 x 2 sampled points\n",
    "\n",
    "    # identical decoder to a plain autoencoder\n",
    "    decoder_input = layers.Input(shape=self.output_dim, name = 'decoder_input')\n",
    "    decoded = self.dense3(encoder_output)\n",
    "    decoded = layers.Reshape(self.shape_before_flattening)(decoded)\n",
    "\n",
    "    decoder_output = self.decoder(decoded)\n",
    "    print(f\"decoder_output_shape = {decoder_output.shape}\")\n",
    "    return decoder_output\n",
    "\n",
    "input_shape = x_test.shape[1:]\n",
    "n_layers_encoder = 4\n",
    "n_layers_decoder = 4\n",
    "output_dim = 2\n",
    "\n",
    "# Joining the Encoder to the Decoder\n",
    "autoencoder = Autoencoder(input_shape, n_layers_encoder, n_layers_decoder,\n",
    "                          output_dim)\n"
   ],
   "metadata": {
    "id": "uRsbgslPCsYu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Loss Function (Reconstruction Loss + KL Divergence Loss)"
   ],
   "metadata": {
    "id": "QW4_-9mMMiYb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# compilation\n",
    "recon_loss_factor = 1000\n",
    "def vae_loss(y_true, y_pred):\n",
    "  print(f\"y_true_shape = {y_true.shape}\")\n",
    "  print(f\"y_pred_shape = {y_pred.shape}\")\n",
    "  RMSE_recon_loss = tf.math.reduce_mean(backend.square(y_true - y_pred))\n",
    "  print(f\"RMSE_recon_loss = {RMSE_recon_loss}\")\n",
    "  kl_loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "  return (recon_loss_factor * RMSE_recon_loss) + kl_loss"
   ],
   "metadata": {
    "id": "pPaJlrbYT8wO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = Adam(learning_rate = 0.0005)\n",
    "autoencoder.compile(optimizer = optimizer, loss = vae_loss)\n",
    "                    # metrics = [vae_recon_loss, vae_kl_loss])"
   ],
   "metadata": {
    "id": "j9mN59RFUZBp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model"
   ],
   "metadata": {
    "id": "WJd53VZZaKrt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "autoencoder.fit(x = x_train, y = x_train,\n",
    "                batch_size = 32,\n",
    "                shuffle = True,\n",
    "                epochs = 10,\n",
    "                validation_data=(x_test, x_test))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2olyk3bpZvTD",
    "outputId": "385459bd-9bf5-4871-aa7a-8865d8d7a104"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "(7, 7, 64)\n",
      "decoder_output_shape = (32, 28, 28, 1)\n",
      "y_true_shape = (32, 28, 28)\n",
      "y_pred_shape = (32, 28, 28)\n",
      "RMSE_recon_loss = Tensor(\"vae_loss/Mean:0\", shape=(), dtype=float32)\n",
      "(7, 7, 64)\n",
      "decoder_output_shape = (32, 28, 28, 1)\n",
      "y_true_shape = (32, 28, 28)\n",
      "y_pred_shape = (32, 28, 28)\n",
      "RMSE_recon_loss = Tensor(\"vae_loss/Mean:0\", shape=(), dtype=float32)\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 40.6458(7, 7, 64)\n",
      "decoder_output_shape = (None, 28, 28, 1)\n",
      "y_true_shape = (None, 28, 28)\n",
      "y_pred_shape = (None, 28, 28)\n",
      "RMSE_recon_loss = Tensor(\"vae_loss/Mean:0\", shape=(), dtype=float32)\n",
      "1875/1875 [==============================] - 117s 61ms/step - loss: 40.6458 - val_loss: 34.5840\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 113s 60ms/step - loss: 34.1599 - val_loss: 33.2883\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 114s 61ms/step - loss: 33.0558 - val_loss: 32.2479\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 113s 60ms/step - loss: 32.3497 - val_loss: 31.9036\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 113s 60ms/step - loss: 31.8671 - val_loss: 31.5748\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 114s 61ms/step - loss: 31.4804 - val_loss: 31.2249\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 113s 60ms/step - loss: 31.1086 - val_loss: 31.5310\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 114s 61ms/step - loss: 30.8475 - val_loss: 30.8191\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 113s 60ms/step - loss: 30.5787 - val_loss: 30.2793\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 113s 60ms/step - loss: 30.3579 - val_loss: 30.0875\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x78db5f9223b0>"
      ]
     },
     "metadata": {},
     "execution_count": 216
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "autoencoder.decoder.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ORzd9at9b_A",
    "outputId": "64583e15-57fa-4396-b5e6-c810ed9565c8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_transpose_120 (Conv  (None, 7, 7, 64)          36928     \n",
      " 2DTranspose)                                                    \n",
      "                                                                 \n",
      " conv2d_transpose_121 (Conv  (None, 14, 14, 64)        36928     \n",
      " 2DTranspose)                                                    \n",
      "                                                                 \n",
      " conv2d_transpose_122 (Conv  (None, 28, 28, 32)        18464     \n",
      " 2DTranspose)                                                    \n",
      "                                                                 \n",
      " conv2d_transpose_123 (Conv  (None, 28, 28, 1)         289       \n",
      " 2DTranspose)                                                    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 92609 (361.75 KB)\n",
      "Trainable params: 92609 (361.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using VAEs to Generate Faces\n",
    "**What's different from the digits example:**\n",
    "1. The faces data has 3 input channels (RGB) instead of just 1. This means that the final convolutional transpose layer of the decoder needs to have 3 channels. Started in colour, ends in colour.\n",
    "\n",
    "2. There will be 200 dimensions in the latent space instead of just 2. More dimensions means more features/complexity/detail will be encoded.\n",
    "\n",
    "3. Batch normalization after each convolution layer to speed up training. Dropout is also used to prevent overfitting.\n",
    "\n",
    "4. The reconstruction loss factor is increased to 10000. 10000 was found to generate good results.  \n",
    "\n",
    "5. We use a generator to feed images to the VAE from a folder, rather than loading all the images into memory first."
   ],
   "metadata": {
    "id": "HgTyIsB86vDR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {
    "id": "D_kE85W16no6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image as PImage\n",
    "from os import listdir"
   ],
   "metadata": {
    "id": "Y_MivQUraShM",
    "ExecuteTime": {
     "end_time": "2024-02-12T16:39:15.044441100Z",
     "start_time": "2024-02-12T16:39:15.027646200Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "DATA_FOLDER = 'img_align_celeba_dir'\n",
    "NUM_IMAGES = 202599\n",
    "INPUT_SHAPE = (64, 64, 3)\n",
    "BATCH_SIZE = 32\n",
    "LATENT_DIM = 200"
   ],
   "metadata": {
    "id": "dL7h0RgW6is5",
    "ExecuteTime": {
     "end_time": "2024-02-12T16:40:52.238606200Z",
     "start_time": "2024-02-12T16:40:52.216265500Z"
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "data_flow = data_gen.flow_from_directory(DATA_FOLDER,\n",
    "                                         target_size = INPUT_SHAPE[:2],\n",
    "                                         batch_size = BATCH_SIZE,\n",
    "                                         shuffle = True,\n",
    "                                         class_mode = 'input',\n",
    "                                         interpolation = 'bilinear')"
   ],
   "metadata": {
    "id": "DUSEYlJV6_ol",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ef70a52e-d770-4c13-8129-079f422c1a96",
    "ExecuteTime": {
     "end_time": "2024-02-12T16:40:59.771247Z",
     "start_time": "2024-02-12T16:40:52.518898900Z"
    }
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 images belonging to 1 classes.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Architecture"
   ],
   "metadata": {
    "id": "QOf0roqD8EP8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from numpy.core.fromnumeric import nonzero\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, encoder_input_shape, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.encoder_input_shape = encoder_input_shape\n",
    "    self.latent_dim = latent_dim\n",
    "    self.shape_before_flattening = (8, 8, 64)\n",
    "    self.dense1 = layers.Dense(self.latent_dim, name = 'mu')\n",
    "    self.dense2 = layers.Dense(self.latent_dim, name = 'log_var')\n",
    "    self.dense3 = layers.Dense(np.prod(self.shape_before_flattening))\n",
    "\n",
    "    ### The encoder\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Input(shape=self.encoder_input_shape, name='encoder_input'),\n",
    "      layers.Conv2D(filters = 32, kernel_size = (3, 3), strides = 2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25)\n",
    "    ])\n",
    "\n",
    "    ### The decoder\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Conv2DTranspose(filters = 64, kernel_size = (3, 3), strides=2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2DTranspose(filters = 64, kernel_size = (3, 3), strides=2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2DTranspose(filters = 32, kernel_size = (3, 3), strides=2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2DTranspose(filters = 3,  kernel_size = (3, 3), strides=2, padding = 'same', activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    print(f'input_shape = {x.shape}')\n",
    "    encoded = self.encoder(x)\n",
    "    self.shape_before_flattening = backend.int_shape(encoded)[1:]\n",
    "    print(self.shape_before_flattening)\n",
    "    encoded = layers.Flatten()(encoded)\n",
    "    mu = self.dense1(encoded)\n",
    "    log_var = self.dense2(encoded)\n",
    "    \n",
    "    def sampling(args):\n",
    "      # Samples a random point from the normal distribution (for a specific mu and log_var)\n",
    "      func_mu, func_log_var = args\n",
    "      epsilon = backend.random_normal(shape = backend.shape(func_mu), mean = 0, stddev = 1.) # random point in normal distribution\n",
    "      return mu + backend.exp(func_log_var / 2) * epsilon\n",
    "\n",
    "    encoder_output = layers.Lambda(sampling, name = 'encoder_output')([mu, log_var]) # samples a point from each distribution\n",
    "    print(f'encoder_output_shape = {encoder_output.shape}')\n",
    "\n",
    "    # identical decoder to a plain autoencoder\n",
    "    decoder_input = layers.Input(shape=self.latent_dim, name = 'decoder_input')\n",
    "    decoded = self.dense3(encoder_output)\n",
    "    print(f'decoded_shape_b4_reshape = {decoded.shape}')\n",
    "    decoded = layers.Reshape(self.shape_before_flattening)\n",
    "    print(f'decoded_shape_after_reshape = {decoded.shape}')\n",
    "    decoder_output = self.decoder(decoded)\n",
    "    print(f'decoder_output_shape = {decoder_output.shape}')\n",
    "    return decoder_output\n"
   ],
   "metadata": {
    "id": "WecGAq-l8GB8",
    "ExecuteTime": {
     "end_time": "2024-02-12T17:16:39.165951100Z",
     "start_time": "2024-02-12T17:16:39.144791500Z"
    }
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build the model\n",
    "VAE = Autoencoder(INPUT_SHAPE, LATENT_DIM)"
   ],
   "metadata": {
    "id": "_Gw9e8CXAbdW",
    "ExecuteTime": {
     "end_time": "2024-02-12T17:12:27.230394200Z",
     "start_time": "2024-02-12T17:12:27.101457200Z"
    }
   },
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "id": "_pkdkkGx_P8v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# compilation\n",
    "recon_loss_factor = 10000\n",
    "def vae_loss(y_true, y_pred):\n",
    "  print(f\"y_true_shape = {y_true.shape}\")\n",
    "  print(f\"y_pred_shape = {y_pred.shape}\")\n",
    "  RMSE_recon_loss = tf.math.reduce_mean(backend.square(y_true - y_pred))\n",
    "  print(f\"RMSE_recon_loss = {RMSE_recon_loss}\")\n",
    "  kl_loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "  return (recon_loss_factor * RMSE_recon_loss) + kl_loss"
   ],
   "metadata": {
    "id": "VJd72le_OkTO",
    "ExecuteTime": {
     "end_time": "2024-02-12T17:12:27.614687500Z",
     "start_time": "2024-02-12T17:12:27.609155600Z"
    }
   },
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = Adam(learning_rate = 0.0005)\n",
    "VAE.compile(optimizer = optimizer, loss = vae_loss)"
   ],
   "metadata": {
    "id": "jDQ544sm_SLf",
    "ExecuteTime": {
     "end_time": "2024-02-12T17:12:27.929927300Z",
     "start_time": "2024-02-12T17:12:27.906435500Z"
    }
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "VAE.fit(\n",
    "    x = data_flow,\n",
    "    epochs = 200\n",
    ")"
   ],
   "metadata": {
    "id": "afKFsDWf_UIH",
    "ExecuteTime": {
     "end_time": "2024-02-12T17:12:28.589230300Z",
     "start_time": "2024-02-12T17:12:28.442614600Z"
    }
   },
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape = (32, 64, 64, 3)\n",
      "(4, 4, 64)\n",
      "length of mu = 200\n",
      "length of log_var = 200\n",
      "encoder_output_shape = (32, 200)\n",
      "decoded_shape_b4_reshape = (32, 4096)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer 'autoencoder_9' (type Autoencoder).\n\n'Reshape' object has no attribute 'shape'\n\nCall arguments received by layer 'autoencoder_9' (type Autoencoder):\n  • x=tf.Tensor(shape=(32, 64, 64, 3), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mVAE\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdata_flow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m200\u001B[39;49m\n\u001B[0;32m      4\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Cell \u001B[1;32mIn[51], line 75\u001B[0m, in \u001B[0;36mAutoencoder.call\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdecoded_shape_b4_reshape = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdecoded\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     74\u001B[0m decoded \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mReshape(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape_before_flattening)\n\u001B[1;32m---> 75\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdecoded_shape_after_reshape = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdecoded\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     76\u001B[0m decoder_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(decoded)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdecoder_output_shape = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdecoder_output\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: Exception encountered when calling layer 'autoencoder_9' (type Autoencoder).\n\n'Reshape' object has no attribute 'shape'\n\nCall arguments received by layer 'autoencoder_9' (type Autoencoder):\n  • x=tf.Tensor(shape=(32, 64, 64, 3), dtype=float32)"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating New Faces"
   ],
   "metadata": {
    "id": "4pcnLfvQCoXe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n_to_show = 30\n",
    "\n",
    "new_output_dim = np.random.normal(size = (n_to_show, VAE.output_dim))\n",
    "\n",
    "reconst = VAE.decoder.predict(np.array(new_output_dim))\n",
    "\n",
    "fig = plt.figure(figsize = (18, 5))\n",
    "fig.subplots_adjust(hspace = 0.4, wspace = 0.4)\n",
    "for i in range(n_to_show):\n",
    "  ax = fig.add_subplot(3, 10, i + 1)\n",
    "  ax.imshow(reconst[i, :, :, :])\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "3RmbDcftCqV9"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
