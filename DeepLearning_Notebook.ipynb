{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2efGTzDoaRE9",
    "ExecuteTime": {
     "end_time": "2024-02-17T18:55:59.070951200Z",
     "start_time": "2024-02-17T18:55:52.045915200Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses, Input, backend\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using VAEs to Generate Faces\n",
    "**What's different from the digits example:**\n",
    "1. The faces data has 3 input channels (RGB) instead of just 1. This means that the final convolutional transpose layer of the decoder needs to have 3 channels. Started in colour, ends in colour.\n",
    "\n",
    "2. There will be 200 dimensions in the latent space instead of just 2. More dimensions means more features/complexity/detail will be encoded.\n",
    "\n",
    "3. Batch normalization after each convolution layer to speed up training. Dropout is also used to prevent overfitting.\n",
    "\n",
    "4. The reconstruction loss factor is increased to 10000. 10000 was found to generate good results.  \n",
    "\n",
    "5. We use a generator to feed images to the VAE from a folder, rather than loading all the images into memory first."
   ],
   "metadata": {
    "id": "HgTyIsB86vDR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {
    "id": "D_kE85W16no6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image as PImage\n",
    "from os import listdir"
   ],
   "metadata": {
    "id": "Y_MivQUraShM",
    "ExecuteTime": {
     "end_time": "2024-02-17T18:56:07.205475300Z",
     "start_time": "2024-02-17T18:56:07.175230200Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "DATA_FOLDER = 'img_align_celeba_dir'\n",
    "NUM_IMAGES = 202599\n",
    "INPUT_SHAPE = (64, 64, 3)\n",
    "BATCH_SIZE = 32\n",
    "LATENT_DIM = 200"
   ],
   "metadata": {
    "id": "dL7h0RgW6is5",
    "ExecuteTime": {
     "end_time": "2024-02-17T18:56:08.005101100Z",
     "start_time": "2024-02-17T18:56:07.960321800Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "data_flow = data_gen.flow_from_directory(DATA_FOLDER,\n",
    "                                         target_size = INPUT_SHAPE[:2],\n",
    "                                         batch_size = BATCH_SIZE,\n",
    "                                         shuffle = True,\n",
    "                                         class_mode = 'input',\n",
    "                                         interpolation = 'bilinear')"
   ],
   "metadata": {
    "id": "DUSEYlJV6_ol",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ef70a52e-d770-4c13-8129-079f422c1a96",
    "ExecuteTime": {
     "end_time": "2024-02-17T18:56:22.025539700Z",
     "start_time": "2024-02-17T18:56:08.481088400Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 images belonging to 1 classes.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Architecture"
   ],
   "metadata": {
    "id": "QOf0roqD8EP8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from numpy.core.fromnumeric import nonzero\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, encoder_input_shape, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.encoder_input_shape = encoder_input_shape\n",
    "    self.latent_dim = latent_dim\n",
    "    self.shape_before_flattening = (4, 4, 64)\n",
    "    self.dense1 = layers.Dense(self.latent_dim, name = 'mu')\n",
    "    self.dense2 = layers.Dense(self.latent_dim, name = 'log_var')\n",
    "    self.dense3 = layers.Dense(np.prod(self.shape_before_flattening))\n",
    "\n",
    "    ### The encoder\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Input(shape=self.encoder_input_shape, name='encoder_input'),\n",
    "      layers.Conv2D(filters = 32, kernel_size = (3, 3), strides = 2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25)\n",
    "    ])\n",
    "\n",
    "    ### The decoder\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Conv2DTranspose(filters = 64, kernel_size = (3, 3), strides=2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2DTranspose(filters = 64, kernel_size = (3, 3), strides=2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2DTranspose(filters = 32, kernel_size = (3, 3), strides=2, padding = 'same'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.LeakyReLU(),\n",
    "      layers.Dropout(rate = 0.25),\n",
    "      layers.Conv2DTranspose(filters = 3,  kernel_size = (3, 3), strides=2, padding = 'same', activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    shape_before_flattening = backend.int_shape(encoded)[1:]\n",
    "    encoded = layers.Flatten()(encoded)\n",
    "    mu = self.dense1(encoded)\n",
    "    log_var = self.dense2(encoded)\n",
    "    \n",
    "    def sampling(args):\n",
    "      # Samples a random point from the normal distribution (for a specific mu and log_var)\n",
    "      func_mu, func_log_var = args\n",
    "      epsilon = backend.random_normal(shape = backend.shape(func_mu), mean = 0, stddev = 1.) # random point in normal distribution\n",
    "      return mu + backend.exp(func_log_var / 2) * epsilon\n",
    "\n",
    "    encoder_output = layers.Lambda(sampling, name = 'encoder_output')([mu, log_var]) # samples a point from each distribution\n",
    "\n",
    "    # Update shape_before_flattening before using it\n",
    "    self.shape_before_flattening = shape_before_flattening\n",
    "    \n",
    "    # identical decoder to a plain autoencoder\n",
    "    decoded = self.dense3(encoder_output)\n",
    "    decoded = layers.Reshape(self.shape_before_flattening)(decoded)\n",
    "    \n",
    "    # sequential decoder \n",
    "    decoder_output = self.decoder(decoded)\n",
    "    return decoder_output"
   ],
   "metadata": {
    "id": "WecGAq-l8GB8",
    "ExecuteTime": {
     "end_time": "2024-02-17T20:27:22.377932200Z",
     "start_time": "2024-02-17T20:27:20.540030500Z"
    }
   },
   "execution_count": 100,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build the model\n",
    "VAE = Autoencoder(INPUT_SHAPE, LATENT_DIM)"
   ],
   "metadata": {
    "id": "_Gw9e8CXAbdW",
    "ExecuteTime": {
     "end_time": "2024-02-17T20:27:26.325228700Z",
     "start_time": "2024-02-17T20:27:22.376821100Z"
    }
   },
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "id": "_pkdkkGx_P8v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# compilation\n",
    "recon_loss_factor = 10000\n",
    "def vae_loss(y_true, y_pred):\n",
    "  RMSE_recon_loss = tf.math.reduce_mean(backend.square(y_true - y_pred))\n",
    "  kl_loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "  return (recon_loss_factor * RMSE_recon_loss) + kl_loss"
   ],
   "metadata": {
    "id": "VJd72le_OkTO",
    "ExecuteTime": {
     "end_time": "2024-02-17T20:27:28.466759Z",
     "start_time": "2024-02-17T20:27:28.452434400Z"
    }
   },
   "execution_count": 102,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = Adam(learning_rate = 0.0005)\n",
    "VAE.compile(optimizer = optimizer, loss = vae_loss)"
   ],
   "metadata": {
    "id": "jDQ544sm_SLf",
    "ExecuteTime": {
     "end_time": "2024-02-17T20:27:29.455227700Z",
     "start_time": "2024-02-17T20:27:29.415444600Z"
    }
   },
   "execution_count": 103,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(len(data_flow)):\n",
    "  epoch_loss = []\n",
    "  print(f\"Epoch: {epoch}\")\n",
    "  batch_no = 0\n",
    "  for batch in data_flow:\n",
    "    batch_no += 1\n",
    "    print(f'On batch {batch_no} / {len(data_flow)}')\n",
    "    images = batch  # If input and output are the same, images serve as both input and target data\n",
    "    with tf.GradientTape() as tape:\n",
    "      reconstructed_images = VAE(images[0])\n",
    "      loss = vae_loss(images[0], reconstructed_images)\n",
    "      batch_loss = tf.reduce_mean(loss)\n",
    "      epoch_loss.append(batch_loss)\n",
    "    gradients = tape.gradient(batch_loss, VAE.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, VAE.trainable_variables))\n",
    "  print(f'epoch_loss = {np.mean(epoch_loss)}')"
   ],
   "metadata": {
    "id": "afKFsDWf_UIH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating New Faces"
   ],
   "metadata": {
    "id": "4pcnLfvQCoXe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n_to_show = 30\n",
    "\n",
    "new_output_dim = np.random.normal(size = (n_to_show, VAE.latent_dim))\n",
    "\n",
    "reconst = VAE.decoder.predict(np.array(new_output_dim))\n",
    "\n",
    "fig = plt.figure(figsize = (18, 5))\n",
    "fig.subplots_adjust(hspace = 0.4, wspace = 0.4)\n",
    "for i in range(n_to_show):\n",
    "  ax = fig.add_subplot(3, 10, i + 1)\n",
    "  ax.imshow(reconst[i, :, :, :])\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "3RmbDcftCqV9",
    "ExecuteTime": {
     "end_time": "2024-02-17T20:38:04.116898800Z",
     "start_time": "2024-02-17T20:38:03.886608800Z"
    }
   },
   "execution_count": 109,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_21' (type Sequential).\n    \n    Input 0 of layer \"conv2d_transpose_40\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 200)\n    \n    Call arguments received by layer 'sequential_21' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 200), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[109], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m n_to_show \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m30\u001B[39m\n\u001B[0;32m      3\u001B[0m new_output_dim \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mnormal(size \u001B[38;5;241m=\u001B[39m (n_to_show, VAE\u001B[38;5;241m.\u001B[39mlatent_dim))\n\u001B[1;32m----> 5\u001B[0m reconst \u001B[38;5;241m=\u001B[39m \u001B[43mVAE\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_output_dim\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m fig \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mfigure(figsize \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m18\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n\u001B[0;32m      8\u001B[0m fig\u001B[38;5;241m.\u001B[39msubplots_adjust(hspace \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.4\u001B[39m, wspace \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.4\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file73zo6lkv.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\jorda\\anaconda3\\envs\\GenerativeDL\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_21' (type Sequential).\n    \n    Input 0 of layer \"conv2d_transpose_40\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 200)\n    \n    Call arguments received by layer 'sequential_21' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 200), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
